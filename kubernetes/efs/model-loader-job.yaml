apiVersion: batch/v1
kind: Job
metadata:
  name: model-loader
  namespace: default
spec:
  ttlSecondsAfterFinished: 300  # Cleanup 5 min after completion
  backoffLimit: 2
  template:
    spec:
      restartPolicy: Never
      
      # Run on any node (doesn't need GPU)
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      
      containers:
        - name: model-loader
          image: vllm/vllm-openai:v0.13.0
          command:
            - python3
            - -c
            - |
              from huggingface_hub import snapshot_download
              import os
              
              model_name = "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
              cache_dir = "/mnt/models/model-cache"
              
              # Check if model already exists
              model_path = os.path.join(cache_dir, f"models--{model_name.replace('/', '--')}")
              if os.path.exists(model_path):
                  print(f"Model already exists at {model_path}")
                  # Verify it has snapshots
                  snapshots_dir = os.path.join(model_path, "snapshots")
                  if os.path.exists(snapshots_dir) and os.listdir(snapshots_dir):
                      print("Model cache is valid, skipping download")
                      exit(0)
              
              print(f"Downloading {model_name} to {cache_dir}...")
              snapshot_download(model_name, cache_dir=cache_dir)
              print("Download complete!")
          
          resources:
            requests:
              memory: 4Gi
              cpu: 2
            limits:
              memory: 8Gi
          
          volumeMounts:
            - name: model-storage
              mountPath: /mnt/models
      
      volumes:
        - name: model-storage
          persistentVolumeClaim:
            claimName: efs-models-pvc
