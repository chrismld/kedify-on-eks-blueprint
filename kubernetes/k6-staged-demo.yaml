apiVersion: v1
kind: ConfigMap
metadata:
  name: k6-staged-demo-script
  namespace: default
data:
  staged-demo.js: |
    import http from 'k6/http';
    import { check, sleep } from 'k6';

    // Staged Demo Load Test for vLLM Autoscaling
    // 
    // This test demonstrates KEDA + Karpenter scaling in 3 stages:
    // 
    // Stage 1 (0-1m):   Light load → triggers first scale-up (1→2 pods)
    // Wait (1-6m):      Karpenter provisions first GPU node (~4-5 min)
    // Stage 2 (6-7m):   Medium load → triggers aggressive scale-up (2→10 pods)
    // Wait (7-12m):     Karpenter provisions remaining nodes (~4-5 min)
    // Stage 3 (12-14m): Full sustained load for 2 minutes
    //
    // Total duration: ~14 minutes

    export const options = {
      scenarios: {
        staged_demo: {
          executor: 'ramping-vus',
          startVUs: 0,
          stages: [
            { duration: '15s', target: 20 },
            { duration: '30s', target: 20 },
            { duration: '15s', target: 50 },
            { duration: '1m', target: 50 },
            { duration: '15s', target: 100 },
            { duration: '2m', target: 100 },
            { duration: '15s', target: 150 },
            { duration: '3m', target: 150 },
            { duration: '30s', target: 0 },
          ],
          gracefulRampDown: '30s',
        },
      },
      thresholds: {
        http_req_duration: ['p(95)<30000'],  // 95% under 30s (generous for cold starts)
        http_req_failed: ['rate<0.3'],       // Allow some failures during scaling
      },
    };

    const prompts = [
      'Explain the concept of autoscaling in Kubernetes in one paragraph.',
      'What are the benefits of using GPU instances for machine learning?',
      'Describe how KEDA works with custom metrics.',
      'What is Karpenter and how does it help with node provisioning?',
      'Explain the difference between horizontal and vertical scaling.',
      'How do inference servers handle concurrent requests?',
      'What are the key metrics to monitor for LLM serving?',
      'Describe the architecture of a typical ML inference pipeline.',
    ];

    export default function () {
      const url = 'http://vllm-service.default.svc.cluster.local:8000/v1/completions';
      const prompt = prompts[Math.floor(Math.random() * prompts.length)];
      
      const payload = JSON.stringify({
        model: 'TheBloke/Mistral-7B-Instruct-v0.2-AWQ',
        prompt: prompt,
        max_tokens: 50,
        temperature: 0.7,
      });

      const params = {
        headers: { 'Content-Type': 'application/json' },
        timeout: '60s',
      };

      const res = http.post(url, payload, params);
      
      check(res, {
        'status is 200': (r) => r.status === 200,
        'has completion': (r) => {
          try {
            const body = JSON.parse(r.body);
            return body.choices && body.choices.length > 0;
          } catch (e) {
            return false;
          }
        },
      });

      sleep(0.1 + Math.random() * 0.2);
    }
---
apiVersion: batch/v1
kind: Job
metadata:
  name: k6-staged-demo
  namespace: default
spec:
  backoffLimit: 0
  ttlSecondsAfterFinished: 300
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: k6
        image: grafana/k6:latest
        command: ["k6", "run", "/scripts/staged-demo.js"]
        volumeMounts:
        - name: k6-script
          mountPath: /scripts
        resources:
          requests:
            cpu: 500m
            memory: 256Mi
          limits:
            cpu: 1000m
            memory: 512Mi
      volumes:
      - name: k6-script
        configMap:
          name: k6-staged-demo-script
