# vLLM Cold Start Optimization: From 14 Minutes to ~2 Minutes

## The Challenge

When you're running LLM inference on Kubernetes, cold starts are your enemy. Every time a new pod spins up, it needs to load a multi-gigabyte model into GPU memory. For our Mistral 7B AWQ model, this meant waiting **14 minutes** just for the model to download from HuggingFace before serving a single request.

In a demo scenario—or any production environment with autoscaling—this is unacceptable. We needed to get this down to minutes, not hours.

This guide documents the journey from 14 minutes to **~2 minutes**, with a focus on operational simplicity and scale-to-zero support.

---

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────┐
│                     GPU Nodes (Karpenter)                   │
│  ┌─────────────────────────────────────────────────────┐   │
│  │ EBS Snapshot (/dev/xvdb)                            │   │
│  │ - Container image (vllm:v0.13.0 ~8-10GB)           │   │
│  │ - Fast image pulls (~10s vs ~90s)                   │   │
│  └─────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
                              +
┌─────────────────────────────────────────────────────────────┐
│                         EFS (Elastic)                       │
│  ┌─────────────────────────────────────────────────────┐   │
│  │ /models/model-cache/                                │   │
│  │ - TheBloke/Mistral-7B-Instruct-v0.2-AWQ (~4GB)     │   │
│  │ - Shared by all pods (ReadWriteMany)               │   │
│  │ - Persists across scale-to-zero                     │   │
│  └─────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
```

**Why this architecture?**

| Concern | EBS Snapshot (per-node) | EFS (shared) |
|---------|------------------------|--------------|
| Scale-to-zero | Model lost when node terminates | Model persists ✓ |
| Model updates | Re-create snapshot, restart nodes | Re-run loader job ✓ |
| Multi-pod | Each node needs snapshot | Shared storage ✓ |
| Cold start | ~24s (from page cache) | ~1-2 min (network) |
| Cost (5 nodes) | ~$70/mo (150GB × 5) | ~$15/mo (EFS + 50GB EBS) |

We chose **EFS for model storage** because scale-to-zero support and operational simplicity outweigh the slightly slower cold starts.

---

## The Optimization Journey

### Stage 1: The Baseline (14 minutes)

Out of the box, vLLM downloads the model from HuggingFace on every pod startup:

```
Loading model from HuggingFace Hub...
Downloading: 100% [████████████████] 3.9GB/3.9GB
Model loading took 840.52 seconds
```

**Problem:** Every pod restart, every scale-up event, every node replacement triggers a fresh download.

---

### Stage 2: Shared Storage with EFS (~1-2 minutes)

Instead of per-node EBS snapshots with the model, we use EFS for shared model storage:

**How it works:**

1. EFS filesystem created by Terraform
2. A one-time Job downloads the model to EFS
3. All vLLM pods mount the same EFS volume
4. Model persists across scale-to-zero events

**Key configuration - EFS PersistentVolumeClaim:**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: efs-models-pvc
spec:
  accessModes:
    - ReadWriteMany  # Multiple pods can read simultaneously
  storageClassName: efs-sc
  resources:
    requests:
      storage: 100Gi  # EFS is elastic, this is just a label
```

**Key configuration - vLLM Deployment volume mount:**

```yaml
volumes:
  - name: model-storage
    persistentVolumeClaim:
      claimName: efs-models-pvc

volumeMounts:
  - name: model-storage
    mountPath: /mnt/models
```

**Result:** Model loads in ~1-2 minutes from EFS. Slower than local disk, but supports scale-to-zero!

---

### Stage 3: Container Image Caching (Optional)

The vLLM container image is ~8-10 GB. On a fresh node, this adds 60-90 seconds to cold start.

We use an EBS snapshot to cache just the container image (not the model):

**Key configuration - EC2NodeClass:**

```yaml
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: gpu
spec:
  blockDeviceMappings:
    - deviceName: /dev/xvdb
      ebs:
        volumeSize: 50Gi  # Just for container images
        volumeType: gp3
        snapshotID: snap-XXXXXXXXX  # Generated by make optimize-vllm
```

**Result:** Container image pull drops from ~90s to ~10s.

---

### Stage 4: Offline Mode + Run:ai Streamer

Even with the model in EFS, vLLM was still reaching out to HuggingFace Hub to verify the model. The fix: force offline mode and use Run:ai Streamer for faster weight loading.

**Key configuration - Environment variables:**

```yaml
env:
  - name: HUGGINGFACE_HUB_CACHE
    value: /mnt/models/model-cache
  - name: HF_HUB_CACHE
    value: /mnt/models/model-cache
  - name: HF_HUB_OFFLINE
    value: "1"
  - name: TRANSFORMERS_OFFLINE
    value: "1"
```

**Key configuration - vLLM args:**

```yaml
args:
  - "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
  - "--load-format"
  - "runai_streamer"  # Concurrent weight streaming
  - "--quantization"
  - "awq"
```

The magic line in the logs confirmed it worked:

```
HF_HUB_OFFLINE is True, replace model_id [TheBloke/Mistral-7B-Instruct-v0.2-AWQ] 
to model_path [/mnt/models/model-cache/models--TheBloke--Mistral-7B-Instruct-v0.2-AWQ/snapshots/...]
Loading safetensors using Runai Model Streamer: 100% Completed
```

---

## Results Summary

| Optimization Stage | Cold Start | Scale-to-Zero | Notes |
|-------------------|------------|---------------|-------|
| Baseline (HF download) | ~14 min | ❌ Re-downloads | Every pod restart |
| EBS Snapshot (model on disk) | ~24s | ❌ Model lost | Fastest, but inflexible |
| **EFS + Image Cache** | **~1-2 min** | **✓ Supported** | Best balance |
| EFS only (no image cache) | ~2-3 min | ✓ Supported | Simplest setup |

**We chose EFS + Image Cache** because:
- Scale-to-zero works (model persists in EFS)
- Model updates are simple (re-run loader job)
- ~1-2 min cold start is acceptable for autoscaling
- Lower cost than per-node EBS snapshots

---

## Complete vLLM Deployment Configuration

Here's the full deployment manifest with EFS storage:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm
  template:
    metadata:
      labels:
        app: vllm
    spec:
      nodeSelector:
        intent: gpu-inference
      
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      
      # Spread pods across nodes
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: vllm
              topologyKey: kubernetes.io/hostname
      
      containers:
        - name: vllm
          image: vllm/vllm-openai:v0.13.0
          
          args:
            - "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
            - "--load-format"
            - "runai_streamer"      # Key: Concurrent weight streaming
            - "--quantization"
            - "awq"
            - "--max-model-len"
            - "4096"
            - "--max-num-seqs"
            - "8"
            - "--gpu-memory-utilization"
            - "0.90"
            - "--enforce-eager"
            - "--tokenizer-mode"
            - "mistral"
          
          env:
            - name: HUGGINGFACE_HUB_CACHE
              value: /mnt/models/model-cache
            - name: HF_HUB_CACHE
              value: /mnt/models/model-cache
            - name: HF_HUB_OFFLINE
              value: "1"                    # Key: Skip HF Hub checks
            - name: TRANSFORMERS_OFFLINE
              value: "1"
          
          resources:
            requests:
              nvidia.com/gpu: 1
              memory: 16Gi
              cpu: 4
            limits:
              nvidia.com/gpu: 1
          
          volumeMounts:
            - name: model-storage
              mountPath: /mnt/models
      
      # Mount EFS for shared model storage
      volumes:
        - name: model-storage
          persistentVolumeClaim:
            claimName: efs-models-pvc
```

---

## Architecture Decision: Why EFS + EBS Image Cache?

We evaluated several approaches for model storage:

| Approach | Cold Start | Scale-to-Zero | Cost (5 nodes) | Complexity |
|----------|------------|---------------|----------------|------------|
| HuggingFace download | ~14 min | ✓ | $0 | Low |
| EBS Snapshot (model) | ~24s | ❌ | ~$70/mo | Medium |
| **EFS + EBS (images)** | **~1-2 min** | **✓** | **~$15/mo** | **Medium** |
| FSx for Lustre | ~30s | ✓ | ~$150/mo | High |
| S3 Express One Zone | ~45s | ✓ | ~$25/mo | Medium |

**EFS won** because:
- Scale-to-zero support (model persists when nodes terminate)
- Shared storage (all pods read from same location)
- Simple model updates (re-run loader job, no snapshot recreation)
- Lower cost than per-node EBS snapshots
- Elastic throughput (no capacity planning)

**We kept EBS snapshots for container images** because:
- vLLM image is ~8-10 GB
- Image pull adds 60-90s without caching
- Snapshot reduces this to ~10s

---

## Instance Type Performance Comparison

Not all GPUs are created equal. Here's how different instance types affect cold start time:

| Instance | GPU | Memory BW | Est. Startup | Best For |
|----------|-----|-----------|--------------|----------|
| g4dn.xlarge | T4 | 320 GB/s | ~1.5-2 min | Budget |
| g4dn.2xlarge | T4 | 320 GB/s | ~1.5-2 min | Tested baseline |
| **g5.xlarge** | A10G | 600 GB/s | ~1-1.5 min | **Best value** |
| g5.2xlarge | A10G | 600 GB/s | ~1-1.5 min | More vCPU |
| g6.xlarge | L4 | 300 GB/s | ~1.5-2 min | Power efficient |

**Note:** With EFS storage, the bottleneck is network throughput, not GPU memory bandwidth. Instance type matters less than with local storage.

---

## The Bottlerocket Snapshot Script

Creating the EBS snapshot requires careful handling of Bottlerocket's limited shell environment. Here's the key command that downloads the model:

```bash
# Run inside Bottlerocket via SSM
apiclient exec admin sheltie ctr \
  -a /run/containerd/containerd.sock \
  -n k8s.io run --rm --net-host \
  --env HF_HOME=/models/model-cache \
---

## Model Loader Job

The model is downloaded to EFS via a Kubernetes Job that runs automatically on first deployment:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: model-loader
spec:
  template:
    spec:
      containers:
        - name: model-loader
          image: vllm/vllm-openai:v0.13.0
          command:
            - python3
            - -c
            - |
              from huggingface_hub import snapshot_download
              snapshot_download("TheBloke/Mistral-7B-Instruct-v0.2-AWQ", 
                              cache_dir="/mnt/models/model-cache")
          volumeMounts:
            - name: model-storage
              mountPath: /mnt/models
      volumes:
        - name: model-storage
          persistentVolumeClaim:
            claimName: efs-models-pvc
```

**Key points:**
- Runs once on first `make deploy-apps`
- Downloads model directly to EFS
- All vLLM pods share the same model cache
- To update the model, delete the job and re-run: `kubectl delete job model-loader && kubectl apply -f kubernetes/efs/model-loader-job.yaml`

---

## What's Left on the Table?

With EFS storage, the ~1-2 minute startup breaks down as:

| Phase | Time | Bottleneck |
|-------|------|------------|
| EFS → CPU (model read) | ~60-90s | EFS throughput (~100-200 MB/s) |
| CPU → GPU transfer | ~15s | PCIe bandwidth |
| Engine initialization | ~4s | CUDA/vLLM overhead |
| KV cache warmup | ~4s | GPU compute |

**To go faster, you'd need:**
- FSx for Lustre (~$150/mo) - reduces model read to ~10-20s
- Keep pods warm (no scale-to-zero) - eliminates cold start entirely
- EBS snapshot with model (loses scale-to-zero support)

For most use cases, ~1-2 minutes is acceptable for autoscaling scenarios.

---

## Quick Start

### New Cluster Setup

```bash
make setup-infra        # Creates cluster + EFS
make build-push-images  # Build API/frontend
make deploy-apps        # Deploy everything + load model to EFS
make optimize-vllm      # Optional: cache container image for faster pulls
```

### Verify It's Working

```bash
# Check model loader completed
kubectl get job model-loader

# Watch vLLM logs
kubectl logs -f deploy/vllm

# You should see:
# HF_HUB_OFFLINE is True, replace model_id [...] to model_path [...]
# Loading safetensors using Runai Model Streamer: 100% Completed
```

---

## Troubleshooting

**Model not found (offline mode fails):**
- Check model loader job: `kubectl logs job/model-loader`
- Verify EFS mount: `kubectl exec deploy/vllm -- ls /mnt/models/model-cache/`
- Look for `models--TheBloke--Mistral-7B-Instruct-v0.2-AWQ` directory

**Model loader job stuck:**
- Check EFS mount targets: `aws efs describe-mount-targets --file-system-id <efs-id>`
- Verify security group allows NFS (port 2049) from EKS nodes

**Still downloading from HuggingFace:**
- Confirm `HF_HUB_OFFLINE=1` is set: `kubectl exec deploy/vllm -- env | grep HF`
- Check logs for "HF_HUB_OFFLINE is True"

**Run:ai Streamer not used:**
- Verify `--load-format runai_streamer` in args
- Check logs for "Loading safetensors using Runai Model Streamer"

**Slow startup despite optimizations:**
- EFS throughput is the bottleneck (~100-200 MB/s for Elastic mode)
- Consider FSx for Lustre if you need faster cold starts
- Or keep pods warm (min replicas > 0)

---

## References

- [NVIDIA Run:ai Model Streamer](https://developer.nvidia.com/blog/reducing-cold-start-latency-for-llm-inference-with-nvidia-runai-model-streamer/)
- [vLLM Documentation](https://docs.vllm.ai/)
- [Amazon EFS CSI Driver](https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html)
- [Karpenter EC2NodeClass](https://karpenter.sh/docs/concepts/nodeclasses/)
- [Bottlerocket on EKS](https://aws.amazon.com/bottlerocket/)
