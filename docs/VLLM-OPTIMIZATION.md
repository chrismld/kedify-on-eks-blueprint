# vLLM Cold Start Optimization: From 14 Minutes to 24 Seconds

## The Challenge

When you're running LLM inference on Kubernetes, cold starts are your enemy. Every time a new pod spins up, it needs to load a multi-gigabyte model into GPU memory. For our Mistral 7B AWQ model, this meant waiting **14 minutes** just for the model to download from HuggingFace before serving a single request.

In a demo scenario—or any production environment with autoscaling—this is unacceptable. We needed to get this down to seconds, not minutes.

This guide documents the journey from 14 minutes to **24 seconds**, with every optimization validated in production on EKS with Karpenter.

---

## The Optimization Journey

### Stage 1: The Baseline (14 minutes)

Out of the box, vLLM downloads the model from HuggingFace on every pod startup:

```
Loading model from HuggingFace Hub...
Downloading: 100% [████████████████] 3.9GB/3.9GB
Model loading took 840.52 seconds
```

**Problem:** Every pod restart, every scale-up event, every node replacement triggers a fresh download.

---

### Stage 2: EBS Snapshot Pre-caching (~3 minutes)

The first major optimization: pre-download the model to an EBS snapshot and attach it to GPU nodes via Karpenter.

**How it works:**

1. A setup script launches a temporary Bottlerocket instance
2. Downloads the model to the instance's data volume (`/local`)
3. Creates an EBS snapshot of that volume
4. Karpenter's EC2NodeClass references this snapshot for GPU nodes
5. New nodes boot with the model already on disk

**Key configuration - EC2NodeClass:**

```yaml
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: gpu
spec:
  blockDeviceMappings:
    - deviceName: /dev/xvdb
      ebs:
        volumeSize: 150Gi
        volumeType: gp3
        iops: 3000
        throughput: 125
        snapshotID: snap-XXXXXXXXX  # Generated by make optimize-vllm
```

**Key configuration - vLLM Deployment volume mount:**

```yaml
volumes:
  - name: model-storage
    hostPath:
      path: /local  # Bottlerocket's data volume
      type: Directory

volumeMounts:
  - name: model-storage
    mountPath: /mnt/models
```

**Result:** No more downloading! But vLLM was still checking HuggingFace and taking ~3 minutes to load.

---

### Stage 3: Offline Mode (~2.5 minutes)

Even with the model cached locally, vLLM was still reaching out to HuggingFace Hub to verify the model. The fix: force offline mode.

**Key configuration - Environment variables:**

```yaml
env:
  - name: HUGGINGFACE_HUB_CACHE
    value: /mnt/models/model-cache
  - name: HF_HUB_CACHE
    value: /mnt/models/model-cache
  - name: HF_HUB_OFFLINE
    value: "1"
  - name: TRANSFORMERS_OFFLINE
    value: "1"
```

The magic line in the logs confirmed it worked:

```
HF_HUB_OFFLINE is True, replace model_id [TheBloke/Mistral-7B-Instruct-v0.2-AWQ] 
to model_path [/mnt/models/model-cache/models--TheBloke--Mistral-7B-Instruct-v0.2-AWQ/snapshots/...]
```

**Result:** Weight loading dropped from 711 seconds to 154 seconds. Progress!

---

### Stage 4: Run:ai Model Streamer (24 seconds)

The final breakthrough came from NVIDIA's Run:ai Model Streamer, which streams model weights concurrently from storage directly into GPU memory.

**Key configuration - vLLM args:**

```yaml
args:
  - "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
  - "--load-format"
  - "runai_streamer"
  - "--quantization"
  - "awq"
  - "--max-model-len"
  - "4096"
  - "--max-num-seqs"
  - "8"
  - "--gpu-memory-utilization"
  - "0.90"
  - "--enforce-eager"
  - "--tokenizer-mode"
  - "mistral"
```

The logs tell the story:

```
Loading safetensors using Runai Model Streamer: 100% Completed | 739/739 [00:01<00:00, 624.61it/s]
[RunAI Streamer] Overall time to stream 3.9 GiB of all files to cpu: 1.24s, 3.1 GiB/s
Model loading took 3.8815 GiB memory and 23.882125 seconds
```

**Result:** 24 seconds total. That's a **35x improvement** from the original 14 minutes.

---

## Results Summary

| Optimization Stage | Weight Loading | Total Startup | Improvement |
|-------------------|----------------|---------------|-------------|
| Baseline (HF download) | 711s | ~14 min | - |
| EBS Snapshot | 711s | ~12 min | 1.2x |
| + Offline Mode | 154s | ~2.5 min | 5.6x |
| + Run:ai Streamer | 1.24s | **24s** | **35x** |

---

## Complete vLLM Deployment Configuration

Here's the full deployment manifest with all optimizations applied:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm
  template:
    metadata:
      labels:
        app: vllm
    spec:
      nodeSelector:
        intent: gpu-inference
      
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      
      containers:
        - name: vllm
          image: vllm/vllm-openai:v0.13.0
          
          args:
            - "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
            - "--load-format"
            - "runai_streamer"      # Key: Concurrent weight streaming
            - "--quantization"
            - "awq"
            - "--max-model-len"
            - "4096"
            - "--max-num-seqs"
            - "8"
            - "--gpu-memory-utilization"
            - "0.90"
            - "--enforce-eager"
            - "--tokenizer-mode"
            - "mistral"
          
          env:
            - name: HUGGINGFACE_HUB_CACHE
              value: /mnt/models/model-cache
            - name: HF_HUB_CACHE
              value: /mnt/models/model-cache
            - name: HF_HUB_OFFLINE
              value: "1"                    # Key: Skip HF Hub checks
            - name: TRANSFORMERS_OFFLINE
              value: "1"
          
          resources:
            requests:
              nvidia.com/gpu: 1
              memory: 16Gi
              cpu: 4
            limits:
              nvidia.com/gpu: 1
          
          volumeMounts:
            - name: model-storage
              mountPath: /mnt/models
      
      volumes:
        - name: model-storage
          hostPath:
            path: /local              # Bottlerocket's data volume
            type: Directory
```

---

## Architecture Decision: Why EBS Snapshots + Bottlerocket?

We evaluated several approaches for pre-caching the model:

| Approach | Pros | Cons |
|----------|------|------|
| **EBS Snapshot** ✓ | Fast attach, works with Karpenter, survives node replacement | Snapshot storage cost (~$6/mo) |
| Local NVMe | Faster I/O | Ephemeral, requires download on each new node |
| Custom AMI | Full control | Maintenance burden, AMI updates |
| S3 + Init Container | Simple | Adds 30-60s download time |

**EBS Snapshots won** because:
- Karpenter can reference them directly in EC2NodeClass
- No custom AMI maintenance—we use AWS-maintained Bottlerocket
- Model persists across node replacements
- Works seamlessly with Spot instances

---

## Instance Type Performance Comparison

Not all GPUs are created equal. Here's how different instance types affect cold start time:

| Instance | GPU | Memory BW | Est. Startup | Best For |
|----------|-----|-----------|--------------|----------|
| g4dn.xlarge | T4 | 320 GB/s | ~24s | Budget |
| g4dn.2xlarge | T4 | 320 GB/s | ~24s ✓ | Tested baseline |
| **g5.xlarge** | A10G | 600 GB/s | ~18-20s | **Best value** |
| g5.2xlarge | A10G | 600 GB/s | ~18-20s | More vCPU |
| g6.xlarge | L4 | 300 GB/s | ~22-24s | Power efficient |
| p4d.24xlarge | A100 | 1,555 GB/s | ~12-15s | Overkill |

**Recommendation:** g5.xlarge offers the best balance of speed and cost—~20% faster than g4dn at 2x the spot price.

---

## The Bottlerocket Snapshot Script

Creating the EBS snapshot requires careful handling of Bottlerocket's limited shell environment. Here's the key command that downloads the model:

```bash
# Run inside Bottlerocket via SSM
apiclient exec admin sheltie ctr \
  -a /run/containerd/containerd.sock \
  -n k8s.io run --rm --net-host \
  --env HF_HOME=/models/model-cache \
  --mount type=bind,src=/local,dst=/models,options=rbind \
  docker.io/vllm/vllm-openai:v0.13.0 model-dl \
  python3 -c 'from huggingface_hub import snapshot_download; \
    snapshot_download("TheBloke/Mistral-7B-Instruct-v0.2-AWQ", \
    cache_dir="/models/model-cache")'
```

**Key points:**
- Uses `sheltie` to access the admin container
- Mounts `/local` (Bottlerocket's data volume) into the container
- Sets `HF_HOME` to write cache to the mounted volume
- The vLLM image already has `huggingface_hub` installed

Run `make optimize-vllm` to execute this automatically.

---

## What's Left on the Table?

After all optimizations, the 24-second startup breaks down as:

| Phase | Time | Bottleneck |
|-------|------|------------|
| Weight streaming (storage → CPU) | 1.24s | EBS throughput (3.1 GB/s) |
| CPU → GPU transfer | ~15s | PCIe bandwidth |
| Engine initialization | ~4s | CUDA/vLLM overhead |
| KV cache warmup | ~4s | GPU compute |

**The remaining time is hardware-bound.** Further improvements would require:
- Faster GPU (H100 with PCIe Gen5)
- Keeping pods warm (no scale-to-zero)
- Smaller model (already using 4-bit quantization)

For most use cases, 24 seconds is excellent—fast enough for responsive autoscaling.

---

## Quick Start

### New Cluster Setup

```bash
make setup-infra        # Creates cluster + snapshot
make build-push-images  # Build API/frontend
make deploy-apps        # Deploy everything
```

### Existing Cluster Optimization

```bash
make optimize-vllm      # Creates snapshot, updates NodeClass, restarts vLLM
```

### Verify It's Working

```bash
# Watch the logs for "Runai Model Streamer"
kubectl logs -f deploy/vllm

# You should see:
# Loading safetensors using Runai Model Streamer: 100% Completed
# [RunAI Streamer] Overall time to stream 3.9 GiB: 1.24s, 3.1 GiB/s
```

---

## Troubleshooting

**Model not found (offline mode fails):**
- Verify snapshot is attached: `kubectl exec deploy/vllm -- ls /mnt/models/model-cache/`
- Check for `models--TheBloke--Mistral-7B-Instruct-v0.2-AWQ` directory

**Still downloading from HuggingFace:**
- Confirm `HF_HUB_OFFLINE=1` is set: `kubectl exec deploy/vllm -- env | grep HF`
- Check logs for "HF_HUB_OFFLINE is True"

**Run:ai Streamer not used:**
- Verify `--load-format runai_streamer` in args
- Check logs for "Loading safetensors using Runai Model Streamer"

**Slow startup despite optimizations:**
- Check instance type (g5 is faster than g4dn)
- Verify EBS volume is gp3 with adequate IOPS

---

## References

- [NVIDIA Run:ai Model Streamer](https://developer.nvidia.com/blog/reducing-cold-start-latency-for-llm-inference-with-nvidia-runai-model-streamer/)
- [vLLM Documentation](https://docs.vllm.ai/)
- [Karpenter EC2NodeClass](https://karpenter.sh/docs/concepts/nodeclasses/)
- [Bottlerocket on EKS](https://aws.amazon.com/bottlerocket/)
